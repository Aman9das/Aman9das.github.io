{
  "hash": "3a3287f273bd424212cfa8b86905f7cc",
  "result": {
    "markdown": "---\ntitle: \"Handling Missing Data in R\"\nsubtitle: \"Expectation Maximisation Algorithm for Bivariate Normal data\"\nauthor: \"Aman Das\"\ndate: 2023-9-4\nformat: html\nimage: missing-data-jigsaw.jpg\ncode-fold: true\n---\n\n::: {.cell}\n\n:::\n\n\nIn this article we compute Maximum-Likelihood Estimate\nfor the parameters of a Bivariate Normal distribution.\n\nWe also learn to handle missing data points in our data set using Expectation\nMaximization Algorithm. \n\n## Theory\n\n### Likelihood\n\nSay $\\vec{X} \\sim f( \\vec{x} \\mid \\theta)$ .\n\nSay we have a sample of $X$: $\\vec{x}$\n\nThen likelihood function $L(\\theta \\mid \\vec{x})$ is a measure of how likely the\noccurrence of the sample is for a given theta.\n\n$$\nL(\\theta \\mid \\vec{x}) = f(\\vec{x} \\mid \\theta)\n$$\n\nOften log-likelihood $\\log(L(\\theta \\mid \\vec{x}))$ denoted by $l(\\theta \\mid \\vec{x})$)\nis used due to convenience.\n\n### Estimate\n\nSay $\\vec{X_1}, \\vec{X_2} \\ldots \\vec{X_n} \\sim f(\\vec{x} \\mid \\theta)$ .\n\nBut $\\theta$ is not known. We need to find an estimate of $\\theta$ based\non the known sample data.\n\nThus the estimate is some function \n$T(\\vec{X_1}, \\vec{X_2} \\ldots \\vec{X_n}) = \\hat{\\theta}$ .\n\nThere are various ways to judge the usefulness of our estimated $\\hat{\\theta}$.\n\n### Maximum Likelihood Estimate\n\nThe estimate $\\hat{\\theta}$ such that likelihood $L(\\hat{\\theta} \\mid \\vec{x})$ is maximzised is\nuseful in many scenarios. We call such estimates as  Maximum Likelihood Estimate (MLE of $\\hat{\\theta_{MLE}}$).\n\nMLE may be computed using a combination of log-likelihood, Algebra, Calculus and other techniques.\n\n### Expectation Maximization Algorithm\n\nSometimes, there are missing data points in $\\vec{x_i}$. In such cases we may not\nbe able to compute the MLE analytically. Thus a numerical approach called \nExpectation Maximization Algorithm is used.\n\n## Data Wrangling\n\n### Import Dataset\n\nThe data is on \"fasting\" (F) and \"after meal - postprandial\" (PP)\nblood sugar levels (measured in mg/dL) of 20  individuals. For a few individuals,\none of the two blood sugar levels was not measured as per the doctor's \nrecommendation. Assume that (F, PP) measurements are i.i.d. bivariate normal\nwith unknown means, variances and correlation.\n\nData is imported from an Open Document Foundation Spreadsheet [`data.ods`](data.ods).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd = read_ods(\"data.ods\")\n```\n:::\n\n\n### Reformat Dataset\n\nOrder the dataset logically so that dat points of similar missing data are together.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nda = d[ complete.cases(d), ]\ndb = d[ !complete.cases(d), ]\ndba = db[ complete.cases(db[ , 1]), ]\ndbb = db[ complete.cases(db[ , 2]), ]\n\nl1 = length(da)\nl2 = length(dba)\nl3 = length(dbb)\n\nd = bind_rows(da, dba, dbb)\ndb = bind_rows(dba, dbb)\nkable(d)\n```\n\n::: {.cell-output-display}\n|  F|  PP|\n|--:|---:|\n| 78| 128|\n| 89| 118|\n| 93| 134|\n| 76| 117|\n| 85| 130|\n| 84| 122|\n| 86| 131|\n| 73| 137|\n| 97| 119|\n| 88| 123|\n| 79| 135|\n| 91|  NA|\n| 77|  NA|\n| 95|  NA|\n| 92|  NA|\n| 81|  NA|\n| NA| 127|\n| NA| 129|\n| NA| 125|\n| NA| 121|\n:::\n:::\n\n\n## Expectation Maximisation Algorithm\n\n### OLS estimate for known dataset\n\nThe distribution is considered to be bivariate normal.\nThus the distribution parameters are \n$\\mu_X$, $\\mu_Y$, $\\sigma^2_X$, $\\sigma^2_Y$ and $\\rho$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nestbvn = function(d) {\n  td = t(d)\n  \n  X = td[1, ]\n  Y = td[2, ]\n  \n  l = length(X)\n  \n  mux = mean(X)\n  muy = mean(Y)\n  \n  s2x = mean( (X-mux) * (X-mux) )\n  s2y = mean( (Y-muy) * (Y-muy) )\n  s2xy = mean( (X-mux) * (Y-muy) )\n  rho = s2xy / sqrt(s2x * s2y)\n  \n  theta = c(mux, muy, s2y, s2x, rho)\n  return(theta)\n}\n\ntheta = estbvn(da)\n```\n:::\n\n\nThe OLS estimates of the parameters are: 84.3636364, 126.7272727, 47.6528926, 49.1404959, -0.311683\n\n### Imputing Missing values\n\nWe use the interim estimates thus obtained to infer the missing data values.\n\nWe make use of the following result:\n\n$$\nE(Y \\mid x) = \\mu_Y + \\rho \\frac{\\sigma_Y}{\\sigma_X} (x - \\mu_X)\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimptmissed = function(d, t) {\n  db1 = d[ complete.cases(db[ , 1]), ]\n  db2 = d[ complete.cases(db[ , 2]), ]\n  \n  db1[ , 2] = t[2] + ( ( t[5] * sqrt(t[4]) * (db1[ , 1] - t[1]) ) / sqrt(t[3]) )\n  db2[ , 1] = t[1] + ( ( t[5] * sqrt(t[3]) * (db2[ , 2] - t[2]) ) / sqrt(t[4]) )\n  \n  res = bind_rows(db1, db2)\n  return(res)\n}\n\nd2 = bind_rows( da, imptmissed(db, theta) )\n```\n:::\n\n\nThe Imputed data set is as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkable(d2)\n```\n\n::: {.cell-output-display}\n|        F|       PP|\n|--------:|--------:|\n| 78.00000| 128.0000|\n| 89.00000| 118.0000|\n| 93.00000| 134.0000|\n| 76.00000| 117.0000|\n| 85.00000| 130.0000|\n| 84.00000| 122.0000|\n| 86.00000| 131.0000|\n| 73.00000| 137.0000|\n| 97.00000| 119.0000|\n| 88.00000| 123.0000|\n| 79.00000| 135.0000|\n| 91.00000| 124.6268|\n| 77.00000| 129.0579|\n| 95.00000| 123.3608|\n| 92.00000| 124.3103|\n| 81.00000| 127.7919|\n| 84.27993| 127.0000|\n| 83.66607| 129.0000|\n| 84.89379| 125.0000|\n| 86.12150| 121.0000|\n:::\n:::\n\n\n### Iteration\n\nWe continue the process by computing the interim estimates and imputing missing data\nrepeatedly.\n\nWe pray that the data converges. Let us observe:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthetalog = tibble(\n  muX = theta[1],\n  muY = theta[2],\n  sigma2X = theta[3],\n  sigma2Y = theta[4],\n  rho = theta[5]\n)\n\nl = 15\nfor ( i in seq(l) ) {\n  theta = estbvn(d2)\n  \n  thetalog = thetalog %>% add_row(\n    muX = theta[1],\n    muY = theta[2],\n    sigma2X = theta[3],\n    sigma2Y = theta[4],\n    rho = theta[5]\n  )\n  \n  d2 = bind_rows(da, imptmissed(db, theta))\n}\n\nkable(thetalog)\n```\n\n::: {.cell-output-display}\n|      muX|      muY|  sigma2X|  sigma2Y|        rho|\n|--------:|--------:|--------:|--------:|----------:|\n| 84.36364| 126.7273| 47.65289| 49.14050| -0.3116830|\n| 85.14806| 126.2574| 29.44717| 40.65642| -0.3758238|\n| 85.27806| 126.1378| 30.73413| 40.63218| -0.4205278|\n| 85.30227| 126.1021| 31.26110| 40.69501| -0.4353826|\n| 85.30641| 126.0899| 31.43959| 40.71755| -0.4399703|\n| 85.30689| 126.0854| 31.49526| 40.72451| -0.4413570|\n| 85.30682| 126.0838| 31.51250| 40.72655| -0.4417789|\n| 85.30674| 126.0833| 31.51789| 40.72715| -0.4419092|\n| 85.30670| 126.0830| 31.51959| 40.72733| -0.4419501|\n| 85.30668| 126.0830| 31.52014| 40.72738| -0.4419631|\n| 85.30667| 126.0830| 31.52032| 40.72740| -0.4419673|\n| 85.30667| 126.0829| 31.52038| 40.72740| -0.4419687|\n| 85.30666| 126.0829| 31.52040| 40.72740| -0.4419692|\n| 85.30666| 126.0829| 31.52041| 40.72740| -0.4419693|\n| 85.30666| 126.0829| 31.52041| 40.72741| -0.4419694|\n| 85.30666| 126.0829| 31.52041| 40.72741| -0.4419694|\n:::\n:::\n\n\nNotice that each the estimate sequences converged within 15 iterations. We take\nthe limit of these sequences to be our Estimates according to the EM algorithm.\n\nThus our parameter estimates are:\n\n\n\n|      muX|      muY|  sigma2X|  sigma2Y|        rho|\n|--------:|--------:|--------:|--------:|----------:|\n| 85.30666| 126.0829| 31.52041| 40.72741| -0.4419694|\n\n\n\n## Conclusion\n\nTherefore, using an iterative method, one can compute the Maximum Likelihood\nestimates for parameters, even if some data points are missing.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}